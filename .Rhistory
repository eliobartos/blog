Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
for(k in 1:n_sim) {
# Sample episode
episode = simulator(policy)
# Evaluate policy
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
}
update = 1/N * (G - Q)
update[is.na(update)] = 0
Q = Q + update
return(Q)
}
Q = MC_policy_evaluation(states, actions, policy, episode, Q, N)
# Generate new epsilon greedy policy
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/k
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
print(episode)
print(Q)
print(policy)
}
#Round last policy to greedy policy
for(i in 1:n_S)
{
policy[i,] = as.numeric(policy[i,] == max(policy[i,]))
}
return(policy)
}
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
states = c("A")
actions = c("l", "r", "d")
simple_mdp = function(policy) {
episode = list()
episode$states = "A"
episode$actions = sample(colnames(policy),1, prob = policy[1,])
if(episode$actions == "r"){
episode$rewards = 5
} else if(episode$actions == "l") {
episode$rewards = -5
} else {
episode$rewards = sample(c(0, 100),1, prob = c(0.4, 0.6))
}
return(episode)
}
simulator = simple_mdp
n_S = length(states)
n_A = length(actions)
policy = matrix(rep(1/n_A, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(policy) = states
colnames(policy) = actions
Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
k = 1
episode = simulator(policy)
episode
policy
k = 2
episode = simulator(policy)
episode
episode = simulator(policy)
episode
episode = simulator(policy)
episode
episode = simulator(policy)
episode
Q = MC_policy_evaluation(states, actions, policy, episode, Q, N)
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
}
update = 1/N * (G - Q)
update[is.na(update)] = 0
Q = Q + update
return(Q)
}
Q = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q
policy = epsilon_greedy_policy(Q, policy, k)
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/k
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
policy
k = 3
Q
episode = simulator(policy)
episoed
episode
Q = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q
N
GLIE_MC_control = function(states, actions, simulator, n_sim = 1000) {
n_S = length(states)
n_A = length(actions)
policy = matrix(rep(1/n_A, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(policy) = states
colnames(policy) = actions
Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
for(k in 2:n_sim) {
# Sample episode
episode = simulator(policy)
# Evaluate policy
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
}
update = 1/N * (G - Q)
update[is.na(update)] = 0
Q = Q + update
return(list(Q = Q, N = N))
}
out = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q = out$Q
N = out$N
# Generate new epsilon greedy policy
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/k
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
print(episode)
print(Q)
print(policy)
}
#Round last policy to greedy policy
for(i in 1:n_S)
{
policy[i,] = as.numeric(policy[i,] == max(policy[i,]))
}
return(policy)
}
states = c("A")
actions = c("l", "r", "d")
simple_mdp = function(policy) {
episode = list()
episode$states = "A"
episode$actions = sample(colnames(policy),1, prob = policy[1,])
if(episode$actions == "r"){
episode$rewards = 5
} else if(episode$actions == "l") {
episode$rewards = -5
} else {
episode$rewards = sample(c(0, 100),1, prob = c(0.4, 0.6))
}
return(episode)
}
simulator = simple_mdp
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
GLIE_MC_control = function(states, actions, simulator, n_sim = 1000) {
n_S = length(states)
n_A = length(actions)
policy = matrix(rep(1/n_A, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(policy) = states
colnames(policy) = actions
Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
for(k in 2:n_sim) {
# Sample episode
episode = simulator(policy)
# Evaluate policy
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
N_tmp = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A) #Tracks if we visited state, otherwise we dont update it
rownames(N_tmp) = states
colnames(N_tmp) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
N_tmp[episode$states[[i]], episode$actions[[i]]] = 1
}
update = 1/N * (G - Q*N_tmp)
update[is.na(update)] = 0
Q = Q + update
return(list(Q = Q, N = N))
}
out = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q = out$Q
N = out$N
# Generate new epsilon greedy policy
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/k
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
print(episode)
print(Q)
print(policy)
}
#Round last policy to greedy policy
for(i in 1:n_S)
{
policy[i,] = as.numeric(policy[i,] == max(policy[i,]))
}
return(policy)
}
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 20)
GLIE_MC_control = function(states, actions, simulator, n_sim = 1000) {
n_S = length(states)
n_A = length(actions)
policy = matrix(rep(1/n_A, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(policy) = states
colnames(policy) = actions
Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
for(k in 2:n_sim) {
# Sample episode
episode = simulator(policy)
# Evaluate policy
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
N_tmp = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A) #Tracks if we visited state, otherwise we dont update it
rownames(N_tmp) = states
colnames(N_tmp) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
N_tmp[episode$states[[i]], episode$actions[[i]]] = 1
}
update = 1/N * (G - Q*N_tmp)
update[is.na(update)] = 0
Q = Q + update
return(list(Q = Q, N = N))
}
out = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q = out$Q
N = out$N
# Generate new epsilon greedy policy
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/k
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
#print(episode)
#print(Q)
#print(policy)
}
#Round last policy to greedy policy
for(i in 1:n_S)
{
policy[i,] = as.numeric(policy[i,] == max(policy[i,]))
}
return(policy)
}
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 5)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control = function(states, actions, simulator, n_sim = 1000) {
n_S = length(states)
n_A = length(actions)
policy = matrix(rep(1/n_A, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(policy) = states
colnames(policy) = actions
Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
for(k in 2:n_sim) {
# Sample episode
episode = simulator(policy)
# Evaluate policy
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
N_tmp = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A) #Tracks if we visited state, otherwise we dont update it
rownames(N_tmp) = states
colnames(N_tmp) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
N_tmp[episode$states[[i]], episode$actions[[i]]] = 1
}
update = 1/N * (G - Q*N_tmp)
update[is.na(update)] = 0
Q = Q + update
return(list(Q = Q, N = N))
}
out = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q = out$Q
N = out$N
# Generate new epsilon greedy policy
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/sqrt(k)
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
#print(episode)
#print(Q)
#print(policy)
}
#Round last policy to greedy policy
for(i in 1:n_S)
{
policy[i,] = as.numeric(policy[i,] == max(policy[i,]))
}
return(policy)
}
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control = function(states, actions, simulator, n_sim = 1000) {
n_S = length(states)
n_A = length(actions)
policy = matrix(rep(1/n_A, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(policy) = states
colnames(policy) = actions
Q = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(Q) = states
colnames(Q) = actions
N = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(N) = states
colnames(N) = actions
for(k in 2:n_sim) {
# Sample episode
episode = simulator(policy)
# Evaluate policy
MC_policy_evaluation = function(states, actions, policy, episode, Q, N) {
G = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A)
rownames(G) = states
colnames(G) = actions
N_tmp = matrix(rep(0, n_S*n_A), nrow = n_S, ncol = n_A) #Tracks if we visited state, otherwise we dont update it
rownames(N_tmp) = states
colnames(N_tmp) = actions
episode$cum_rewards = rev(cumsum(rev(episode$rewards)))
for(i in 1:length(episode$actions)) {
G[episode$states[[i]], episode$actions[[i]]] = G[episode$states[[i]], episode$actions[[i]]] + episode$cum_rewards[[i]]
N[episode$states[[i]], episode$actions[[i]]] = N[episode$states[[i]], episode$actions[[i]]] + 1
N_tmp[episode$states[[i]], episode$actions[[i]]] = 1
}
update = 1/N * (G - Q*N_tmp)
update[is.na(update)] = 0
Q = Q + update
return(list(Q = Q, N = N))
}
out = MC_policy_evaluation(states, actions, policy, episode, Q, N)
Q = out$Q
N = out$N
# Generate new epsilon greedy policy
epsilon_greedy_policy = function(Q, policy, k) {
epsilon = 1/k
n_s = nrow(policy)
n_a = ncol(policy)
for(i in 1:n_s) {
policy[i,which.max(Q[i,])] = epsilon/n_a + 1 - epsilon
policy[i,-which.max(Q[i,])] = epsilon/n_a
}
return(policy)
}
policy = epsilon_greedy_policy(Q, policy, k)
#print(episode)
#print(Q)
#print(policy)
}
#Round last policy to greedy policy
for(i in 1:n_S)
{
policy[i,] = as.numeric(policy[i,] == max(policy[i,]))
}
return(policy)
}
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
simple_mdp = function(policy) {
episode = list()
episode$states = "A"
episode$actions = sample(colnames(policy),1, prob = policy[1,])
if(episode$actions == "r"){
episode$rewards = 5
} else if(episode$actions == "l") {
episode$rewards = -5
} else {
episode$rewards = sample(c(0, 100),1, prob = c(0.9, 0.1))
}
return(episode)
}
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 1000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 10000)
GLIE_MC_control(states, actions, simple_mdp, n_sim = 100000)
setwd("~/GitHub/eliobartos.github.io")
